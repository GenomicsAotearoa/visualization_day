[
  {
    "objectID": "viz_start_to_finish.html",
    "href": "viz_start_to_finish.html",
    "title": "Start to Finish Visualizing your Data",
    "section": "",
    "text": "If you want to start practicing visualizations, a really great place to start is with tidytuesday. Tidytuesday is a community initiative that publishes a weekly dataset in an easy to access format. Every week, people create a visualization of the data and share it on some form of social media (or just keep it to themselves). In 2024 tidytuesday had the aim of being featured in 10+ training courses. They hit more than 30+!\nCheck out the tidytuesday github to see all the data.\nIn this episode we are going to access some of the tidytuesday data and go through the process of visualizing the data. There are hundreds of example datasets available, and some of them are very interesting! Part of the aim of this episode is to explore a dataset that we are not familiar with. This material is based on my first experience participating in tidytuesday, and I had no idea what the data was like or what to expect.",
    "crumbs": [
      "Start to Finish Visualizing your Data"
    ]
  },
  {
    "objectID": "viz_start_to_finish.html#overview",
    "href": "viz_start_to_finish.html#overview",
    "title": "Start to Finish Visualizing your Data",
    "section": "Overview",
    "text": "Overview\nTogether, we are going to go from start to finish and create a visualization. Start to finish means we will need to explore the data, make decisions involving modifications and transformations, and finally, visualize the data in a meaningful way.\nNote: Just the other day I saw a post on LinkedIn, someone had done a screencast recording of them going through a tidytuesday visualization (in Python!). The poster warns us: “I have to admit, this screencast is 80% me orienting to this week’s data”. The comments section immediately responds that “80% orienting to the data is real life!”\n\nAims\nTo demonstrate the full process of exploratory analysis, data transformation, and visualization.\nGenerate discussion, via questions and exercises, about the data and the validity of our modifications and decisions.\nTo generate a figure that is clear, conveys a message, and is visually appealing.\n\n\nHow this will work\nI will work through all the code required to create the end product visualization. You can follow along exactly, or you can opt to deviate from my example (e.g., during data transformation step you might choose to keep 25 samples while I keep 20, you might choose to use the median while I use the mean). If you are newer to ggplot2, then I recommend you follow exactly. If you are more confident, then modify the code as you see fit.\nYou will be provided with a full and complete copy of all this code, to use as a template for your own work.",
    "crumbs": [
      "Start to Finish Visualizing your Data"
    ]
  },
  {
    "objectID": "viz_start_to_finish.html#getting-started",
    "href": "viz_start_to_finish.html#getting-started",
    "title": "Start to Finish Visualizing your Data",
    "section": "Getting started",
    "text": "Getting started\n\nPackages\nFor this section we are going to need dplyr, readr, and ggplot. We can load them separately, or we can load the whole tidyverse package.\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nImporting the data\nAll tidytuesday data is available for easy download and importing. The data is generally very well organised (it has already gone through cleaning and is ‘tidy’).\n\nparfumo_data_clean &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-12-10/parfumo_data_clean.csv')\n\nRows: 59325 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): Number, Name, Brand, Concentration, Main_Accords, Top_Notes, Middl...\ndbl  (3): Release_Year, Rating_Value, Rating_Count\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.",
    "crumbs": [
      "Start to Finish Visualizing your Data"
    ]
  },
  {
    "objectID": "viz_start_to_finish.html#exploratory-analysis",
    "href": "viz_start_to_finish.html#exploratory-analysis",
    "title": "Start to Finish Visualizing your Data",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\nThe first step in exploratory analysis is to understand our data. This is the equivalent of picking up a wrapped present, feeling the weight, how solid it is, maybe giving it a little bit of a shake, and trying to figure out what’s inside. With our data, we are trying to build a light-weight mental model - what each object is storing, whether the data is numeric, are there missing values, are there biases between groups, is the data normally distributed etc., etc.,.\nUseful functions here include dim(), colnames(), head(), summary(), str(), class(), and many others. You will develop a set of favourites and defaults.\n\nparfumo_data_clean |&gt; head()\n\n# A tibble: 6 × 13\n  Number Name         Brand Release_Year Concentration Rating_Value Rating_Count\n  &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;        &lt;dbl&gt;\n1 455    Tabac Écarl… Le R…           NA &lt;NA&gt;                    NA           NA\n2 0071   Tidal Pool   CB I…         2004 &lt;NA&gt;                    NA           NA\n3 0154   Pumpkin Pie  CB I…         1998 &lt;NA&gt;                    NA           NA\n4 0162   Wet Stone    CB I…         2006 &lt;NA&gt;                    NA           NA\n5 0171   Chocolate B… CB I…           NA &lt;NA&gt;                    NA           NA\n6 0191   My Birthday… CB I…         1975 &lt;NA&gt;                    NA           NA\n# ℹ 6 more variables: Main_Accords &lt;chr&gt;, Top_Notes &lt;chr&gt;, Middle_Notes &lt;chr&gt;,\n#   Base_Notes &lt;chr&gt;, Perfumers &lt;chr&gt;, URL &lt;chr&gt;\n\nparfumo_data_clean |&gt; summary()\n\n    Number              Name              Brand            Release_Year  \n Length:59325       Length:59325       Length:59325       Min.   :1709   \n Class :character   Class :character   Class :character   1st Qu.:2005   \n Mode  :character   Mode  :character   Mode  :character   Median :2013   \n                                                          Mean   :2006   \n                                                          3rd Qu.:2018   \n                                                          Max.   :2024   \n                                                          NA's   :20316  \n Concentration       Rating_Value     Rating_Count     Main_Accords      \n Length:59325       Min.   : 0.000   Min.   :   2.00   Length:59325      \n Class :character   1st Qu.: 6.900   1st Qu.:   6.00   Class :character  \n Mode  :character   Median : 7.400   Median :  19.00   Mode  :character  \n                    Mean   : 7.347   Mean   :  60.66                     \n                    3rd Qu.: 7.900   3rd Qu.:  62.00                     \n                    Max.   :10.000   Max.   :2732.00                     \n                    NA's   :29279    NA's   :29279                       \n  Top_Notes         Middle_Notes        Base_Notes         Perfumers        \n Length:59325       Length:59325       Length:59325       Length:59325      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n     URL           \n Length:59325      \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nparfumo_data_clean |&gt; colnames()\n\n [1] \"Number\"        \"Name\"          \"Brand\"         \"Release_Year\" \n [5] \"Concentration\" \"Rating_Value\"  \"Rating_Count\"  \"Main_Accords\" \n [9] \"Top_Notes\"     \"Middle_Notes\"  \"Base_Notes\"    \"Perfumers\"    \n[13] \"URL\"          \n\n\nExercise: Note down your initial conclusions and impressions.\n\nWhat are five things you notice about the data?\nWhat is one question you have about the data? (note: looking for a small, simple question about the dataset, not a hypothesis to test)\nWhat is something that jumps out at you, which you might like to investigate, visualize, or learn about? (This could be a hypothesis to test)\n\n\n\n\n\n\n\nInitial observations and impressions\n\n\n\n\n\nThere are many options, but some things that stood out to me:\n\nThere are NAs! Concentration is almost 80% NA, Rating_Count is almost 50% NA (from the summary() function).\nThere is a mix of numeric and character class data.\nRating_Value looks like it’s a 0-10 scale, with a mean and median around 7.3 - 7.4.\nMeanwhile, the mean and median of Rating_Count are quite different: median of 19 and a mean of 60.\nThe basic structure of the data: perfume name, brand, and then a series of values and then a set of descriptions.\n\nQuestions and assumptions: I’m assuming that Rating_Value is an average, with Rating_Count describing how many individual Ratings contributed to the overall value, but that isn’t explained. It could be a mean or median (or mode!)\n\n\n\n\nBrand Rating\nLooking at the data I was interested in Brand, Rating_Value, and Rating_Count. My aim is to generate a visualization which will show which brands consistently have high ratings for their different products, with the intention that a person with little to no knowledge about perfume (like myself) can get an idea of reliable brands.\nExercise: Mentally visualize what this figure might look like. What are the key components we will need to convey?\n\n\n\n\n\n\nMental visualization\n\n\n\n\n\nNeed to convey the average rating for a brand, as well as the variance. Do not need to visualize all brands, only those with consistently high ratings. Boxplots (and violin plots, jitter plots etc.,) are a good way to show the distribution of scores.\n\n\n\n\nAssessing the means\nA useful place to start will be to look at the average (and here I’ll be using mean) rating value for a brand. To do this we will use the dplyr verbs (functions) to group rows based on which Brand they are from, generate a single summary value (a mean of the Rating_Value) for each Brand, arrange the means from highest to lowest, and then use the head() function to view the output.\n\nparfumo_data_clean |&gt; # Remember that |&gt; is the 'pipe', which passes data\\ to the next function.\n  group_by(Brand) |&gt; # All rows which share the same Brand are now grouped.\n  summarize(avg_Rating = mean(Rating_Value, na.rm = TRUE)) |&gt; # Calculate Brand\\ means\n  arrange(desc(avg_Rating)) |&gt; # Arrange Brands from highest to lowest\n  head()\n\n# A tibble: 6 × 2\n  Brand            avg_Rating\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 Natura                10   \n2 Sarahs Creations      10   \n3 mesOud                 9.47\n4 Bourjois               9.4 \n5 Jehanne Rigaud         9.3 \n6 Max Joacim             9.3 \n\n\nExercise: Discuss these results.\n\n\n\n\n\n\nInterpreting these results\n\n\n\n\n\nNatura and Sarahs Creations share the top score of 10. No need for further analysis, we can all go home and talk about our favourite new perfume brands which are obviously very reliable!\n…Or maybe not.\nSince we use base 10, many surveys and tests tend to be scored out of 10. To see two brands with what look like perfect scores makes me very suspicious, especially given it’s a mean - this would require every single person to have rated every single one of their perfumes as a 10. Either they truly are perfect or, more likely, this is due to a very small number of ratings on a single type of perfume.\n\n\n\nIf you are confident in these results, I recommend you re-run summary() and look at the data. Does it change your opinion?\n\nparfumo_data_clean |&gt; summary()\n\n    Number              Name              Brand            Release_Year  \n Length:59325       Length:59325       Length:59325       Min.   :1709   \n Class :character   Class :character   Class :character   1st Qu.:2005   \n Mode  :character   Mode  :character   Mode  :character   Median :2013   \n                                                          Mean   :2006   \n                                                          3rd Qu.:2018   \n                                                          Max.   :2024   \n                                                          NA's   :20316  \n Concentration       Rating_Value     Rating_Count     Main_Accords      \n Length:59325       Min.   : 0.000   Min.   :   2.00   Length:59325      \n Class :character   1st Qu.: 6.900   1st Qu.:   6.00   Class :character  \n Mode  :character   Median : 7.400   Median :  19.00   Mode  :character  \n                    Mean   : 7.347   Mean   :  60.66                     \n                    3rd Qu.: 7.900   3rd Qu.:  62.00                     \n                    Max.   :10.000   Max.   :2732.00                     \n                    NA's   :29279    NA's   :29279                       \n  Top_Notes         Middle_Notes        Base_Notes         Perfumers        \n Length:59325       Length:59325       Length:59325       Length:59325      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n     URL           \n Length:59325      \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n\nThe summary() function is very useful here. Looking at the column for Rating_Value, I can see that the mean and median values are around 7.3-7.4, and the range is 0 - 10. At this time I’m also looking at the Rating_Count variable, and seeing that the range there is 2 - 2,732. That tells me that some perfumes were only rated twice. I’m very suspicious that the brands Natura and Sarahs Creations have only a single perfume which was rated only two or three times, which allows them to get perfect scores. Similarly, any other brand with a score that seems too good to be true could be the result of this low-sampling bias.\nExercise: Is my hypothesis (very high scores are the result of low sampling bias) true? What type of visualization would help us to test this? In the space below, create a quick plot to ask whether there is a low-sampling bias present.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nI am assuming there is a negative relationship between avg_Rating and Rating_count (fewer ratings allow for a higher avg score). A scatter plot, made with geom_point(), with Rating_Count and Rating_Value on the x and y, is a quick and easy way to visualize a relationship like this.\n\nggplot(data = parfumo_data_clean, \n      mapping = aes(x = Rating_Count, \n                    y = Rating_Value)) +\n  geom_point()\n\nWarning: Removed 29279 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nWarning: 29279 rows removed. Missing values or values outside the scale range - what is this? This warning appears when we attempt to plot values that are outside of the range of the X or Y axis. However, this warning also appears when there is missing data. In this case we know the range of values is 0 - 10, and from the summary() function know that both the Rating variables have 29,279 missing rows. During our initial look at summary(), we noted approx 30,000 missing rows (NAs), leaving approx 30,000 rows of data. If our initial data set didn’t have missing values, this warning would be concerning.\nWarnings can be suppressed, but since we won’t be keeping them in the final plot it’s not critical and I prefer to leave them showing while working through exploratory analysis plots.\n\n\n\nIt seems there is a relationship between Rating_Value and Rating_Count - one we can probably conceptualize quite well! Scents with fewer ratings exhibit the highest and lowest values. Why is this? Presumably all perfumes receive some very high and very low scores based on personal preference. If a perfume has only a small number of ratings, the rating_value is skewed towards those extreme scores. Perfumes with more ratings are gravitating towards a point just below 7.5. This matches with mean and median Rating_Value of 7.35 and 7.40, respectively, which we noted from the summary() function at the start of this episode. It makes sense that as the number of ratings increases, the average rating converges on a middle-ground.\nImportantly, I think this is reasonable grounds to filter our data based on a Rating_Count threshold, since perfumes with a low Rating_Count can be skewed. I need to define a filter threshold, and I’ll use the median value of Rating_Count (19): rows (perfumes) will only be kept if they have been rated 19 times or more.\n\n\nAn aside\nI’ll also take a guess that there will be relationship between Rating_Count and Release_Year. I imagine that newer perfumes will have more ratings due to the internet, marketing, and population. This is important because if we are filtering based on a Rating_Count threshold, we need to be aware this is reducing the likelihood of older Brands appearing.\n\nggplot(data = parfumo_data_clean, \n        mapping = aes(x = Release_Year, y = Rating_Count)) +\n  geom_point()\n\nWarning: Removed 34977 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\nDecisions\nI have decided to filter perfumes/rows based on a criteria of having been rated 19 or more times (based on the median rating_Count of 19). Perfumes rated fewer than 19 times will be excluded from the analysis.\nExercise: Discuss with others - do you agree with the decision to filter like this? Can you propose an alternative method? Is our threshold of 19 reasonable?",
    "crumbs": [
      "Start to Finish Visualizing your Data"
    ]
  },
  {
    "objectID": "viz_start_to_finish.html#transformations",
    "href": "viz_start_to_finish.html#transformations",
    "title": "Start to Finish Visualizing your Data",
    "section": "Transformations",
    "text": "Transformations\nTransforming data is a term that includes grouping data (as we did above, for Brands), arranging (sorting), moving or renaming columns, etc.,. It also applies to filtering, which we are about to perform. I think there is a distinction between sorting rows or renaming columns and filtering, which is the removal of data from further analysis, so I’m giving it a separate subsection.\nRemoving data, setting thresholds, removing outliers, defining groups, choosing methods - all of these decisions can have significant impacts on your final results. We must keep this in mind and always be ready to question our decisions. In bioinformatics it is usually plausible to repeat analyses with different decisions, and if we find that our decisions are having a big impact on our results, we need to be mindful of this.\nIn the callout box below, we move through a whole series of transformation steps. The steps are summarised here, and described in detail in the callout box.\n\nFiltering for low rating counts: Perfumes that were rated fewer than 19 times (the median number of ratings) were removed from the data. Having a small number of ratings can skew the rating result (e.g., a single 10/10 is not representative of the perfumes quality).\nCalculate the average rating by brand and the number of perfumes per brand: Perfumes were grouped by brand, and the average (mean) rating for each brand was calculated. Some brands had only a small number of perfumes, which led to skewed averages.\nRemove brands with a small number of perfumes: Brands with fewer than 20 perfumes were removed from the analysis.\nStore the perfume_brand_data object: A dataframe of 1635 rows, consisting of only perfumes that meet the above two criteria (individually rated more than 19 times, and from a brand with 20 or more perfumes).\nCreate and store the brandRatingData object: A dataframe of 20 rows and three variables, consisting of the 20 brands with the highest average rating (the mean rating across all perfumes for the brand), as well as the number of perfumes in each brand.\n\nWhile these steps are an important part of visualization, they are somewhat repetitive and time-consuming and therefore have been compressed and skipped for the sake of this workflow. You can view all of the code and detailed explanations for what steps were taken in the callout box.\nExercise: The above transformations include two user-defined thresholds which determine what data is included in further analyses. Discuss the thresholds used and whether you believe they are reasonable and logical. What approach would you take in this situation?\nNote: the thresholds, and the logic behind them, are described in more detail in the callout box.\n\n\n\n\n\n\nTransformation steps in detail\n\n\n\n\n\n\nFiltering for low rating counts\nThe summary() function reveals that the median number of Rating_Counts is 19, so I’m choosing to keep only rows which have 19 or more ratings. You may decide to change this number or keep it as is.\nThe code below will use filter() to discard all rows which do not meet the threshold (equal to or greater than 19), and will then repeat the steps we did earlier - grouping perfume by Brand, calculating the average rating for those brands, and returning the highest scores.\n\nparfumo_data_clean |&gt; \n  filter(Rating_Count &gt;= 19 ) |&gt; # keep rows where Rating_Count is\\ greater than or equal to 19.\n  group_by(Brand) |&gt; # Group perfumes by brand\n  summarize(avg_Rating = mean(Rating_Value, na.rm = TRUE)) |&gt; # Calculate a\\ new variable, which is the mean of Rating_Value, label\\ it \"avg_Rating\"\n  arrange(desc(avg_Rating)) |&gt; # Sort the new avg_Rating variable in descending order\n  head()\n\n# A tibble: 6 × 2\n  Brand                      avg_Rating\n  &lt;chr&gt;                           &lt;dbl&gt;\n1 Miyaz Perfume                    8.8 \n2 City Rhythm                      8.7 \n3 Kuschel J / クシェルヨット       8.7 \n4 Snif                             8.7 \n5 Sultan Pasha Attars              8.61\n6 Alafasy                          8.6 \n\n\nThe new average ratings are much more plausible. They are also higher than the mean and median (from the summary() function), which is a good sanity check. If my highest scoring brands were lower than the mean or median, I would know I have made a critical error. This might seem obvious, but it’s important to constantly perform these mental checks.\n\n\n\n\n\n\nSanity Checks\n\n\n\n\n\nSanity checks are a term used in bioinformatics and data science. What are they? Why are they important?\nCompare what we are doing today with a task like cooking a dinner. When cooking we have a constant stream of information from our five senses - we would smell that dairy products had gone too far past their use by date or if something was burning, we can hear the pot boiling over on the stove, feel that the carrot is too woody. In contrast, we have almost no feedback about our data! What’s more, our data is often too large and complex for us to interpret easily - I can’t eyeball a thousand data points and be confident they are normally distributed, for example (well, I could be confident, but that wouldn’t mean I’m correct).\nSanity checks are a way to engage with your data and make sure your results make logical sense. Examples include, but are in no way limited to: confirming data types (dates in date columns) and plausible ranges (human age should be between 0 - 120), checking distributions (especially if a statistical method assumes normality, but also for the presence of outliers), validate relationships (longer genes should probably have more mutations than shorter genes, larger genomes should have larger filesizes), data volume (rows and columns should remain the same between steps - unless you are filtering, treatment vs control groups should have equal numbers).\nSanity checks are an entire arm of bioinformatics and data science, much the same way good visualization is. It’s a skill that takes time and effort to build.\n\n\n\n\n\nTransformations cont\nWe’ve taken the step of removing perfumes that had too few ratings to be accurate. Since we are investigating Brands, rather than individual perfumes, it’s logical to ask whether any Brands are scoring highly because they have only one or two perfumes that perform well (a “one-hit-wonder” brand).\nTo test this hypothesis, we can take advantage of the fact that the summarize() function can simultaneously calculate multiple summaries. We will re-run the code from above, but this time include an additional line so that we can see both the average rating and the number of perfumes contributing to the rating.\n\nparfumo_data_clean |&gt; \n  filter(Rating_Count &gt;= 19 ) |&gt; \n  group_by(Brand) |&gt; \n  summarize(avg_Rating = mean(Rating_Value, na.rm = TRUE),\n            number_Perfumes = n()) |&gt; # The \"n()\" counts the number of rows in\\ the group. \n  arrange(desc(avg_Rating)) |&gt; \n  head(n = 10)\n\n# A tibble: 10 × 3\n   Brand                       avg_Rating number_Perfumes\n   &lt;chr&gt;                            &lt;dbl&gt;           &lt;int&gt;\n 1 Miyaz Perfume                     8.8                2\n 2 City Rhythm                       8.7                1\n 3 Kuschel J / クシェルヨット        8.7                1\n 4 Snif                              8.7                1\n 5 Sultan Pasha Attars               8.61               7\n 6 Alafasy                           8.6                1\n 7 Nilafar du Nil                    8.6                2\n 8 Mind Games                        8.58               6\n 9 MAD Parfumeur                     8.57               3\n10 Arabian Oud / العربية للعود       8.55               2\n\n\nI can see an issue here - some of these brands have only one or two perfumes. Conceivably, it would be difficult for brands with e.g., 100 perfumes to consistently receive such high ratings, and they will therefore not show up in this list. This is essentially the same issue dealt with above - having fewer ratings, or fewer products, means an average score can be skewed.\nTo deal with this, I’ve added an arbitrary filtering threshold for a brand to have 20 or more perfumes in order to be considered.\n\nparfumo_data_clean |&gt; \n  filter(Rating_Count &gt;= 19 ) |&gt; \n  group_by(Brand) |&gt; \n  summarize(avg_Rating = round(mean(Rating_Value, na.rm = TRUE), 1),\n            number_Perfumes = n()) |&gt; # summarize() can create multiple columns simultaneously\\ here creating both avg_Rating and number_Perfumes\n  filter(number_Perfumes &gt;= 20) |&gt; \n  arrange(desc(avg_Rating)) |&gt; \n  head(n = 10)\n\n# A tibble: 10 × 3\n   Brand                avg_Rating number_Perfumes\n   &lt;chr&gt;                     &lt;dbl&gt;           &lt;int&gt;\n 1 Ensar Oud / Oriscent        8.4              93\n 2 Nabeel                      8.3              22\n 3 Aaron Terence Hughes        8.1              61\n 4 Annette Neuffer             8.1              20\n 5 Chanel                      8.1              83\n 6 Clive Christian             8.1              45\n 7 Roja Parfums                8.1             118\n 8 Widian / AJ Arabia          8.1              21\n 9 Areej Le Doré               8                24\n10 Amouage                     7.9              92\n\n\nExercise: Discuss the legitimacy of this filtering threshold of 20. How does this compare to our previous filtering? What would you do differently?\n\n\n\n\n\n\nA ‘practical’ threshold\n\n\n\n\n\nIt might feel like 20 is an arbitrary number for a threshold (and it is). However, I think this is a fine approach, because we are thinking about the practical or translational nature of our question - 20 perfumes is a) not an unreasonable number for a mean and is b) plenty to choose from if a person was in a store - and that’s the goal of this viz.\n\n\n\n\n\nBrand Rating Data\nAn important thing to remember is that the analyses above didn’t actually modify the data - they were only performing the filtering, calculating the summaries, and printing the result to the screen. No data was modified (permanently).\nHere we will generate a new object to store the data after applying our filtering steps.\n\nkeep_brands &lt;- parfumo_data_clean |&gt; \n  filter(Rating_Count &gt;= 19 ) |&gt; \n  group_by(Brand) |&gt; \n  summarize(avg_Rating = round(mean(Rating_Value, na.rm = TRUE), 1),\n            number_Perfumes = n()) |&gt; \n  filter(number_Perfumes &gt;= 20) |&gt; \n  arrange(desc(avg_Rating)) |&gt; \n  head(n = 10)\n\n# Below, filter is checking whether the element in the Brand column is in keep_brands. \nperfume_brand_data &lt;- parfumo_data_clean |&gt; \n  filter(Brand %in% keep_brands$Brand)\n\nkeep_brands |&gt; rm()\n\nExercise: Take five minutes to look at this code and predict the output - do you understand what we have done here? Manipulating objects like this is a critical skill. Take one more look at the code and make sure you understand each line of code and what the output is. Use head(), summary() or any other functions to check the output. Note any observations.\n\n\n\n\n\n\nObservations\n\n\n\n\n\nThe above lines of code are: - Creating a new object, “keep_brands”.\n\nFiltering the parfumo object to keep only those perfumes with a Rating_Count of 19 or greater.\nGrouping perfumes by brand.\nThe summarize() function is creating two new variables: avg_Rating, which is the overall average rating for a Brand and number_Perfumes, which is the number of perfumes that were rated for this brand.\nAnother filter(), this time keeping only those brands with 20 or more perfumes (which have a Rating_Count of 19 or more).\nArrange (sort) the brands by their overall score (from highest to lowest), and keep only the top 10 brands.\n\nkeep_brands is therefore the 10 perfume brands we are interested in: after filtering, they have the highest average rating. Because of how summarize() works, keep_brands only has the new variables we created - avg_Rating and number_Perfumes (plus the Brand names).\nNext, use the Brand variable in keep_brands to pull out all the data from parfumo_data_clean. Here, filter() is being used just like above, but instead of filtering on a “greater than” threshold, we are filtering on whether or not the parfumo_data_clean Brand element is found in the keep_brand object.\nThis has introduced a critical error. It has kept only the brands we want, but we have not specified that we only want those perfumes with a Rating_Count of 19 or more (we have filtered for all perfumes produced by the brands we selected).\nIt is very easy to make a mistake like this\nWhich is why we must always be thinking of sanity checks. The summary() function shows us that the min value in Rating_Count is 2, which is how I realised that somewhere, I’d lost my filtering threshold.\nperfume_brand_data |&gt; summary()\n\n\n\nAnd now we will create a second object which includes our data in an alternative format:\n\nbrandRatingData &lt;- parfumo_data_clean |&gt; \n  filter(Rating_Count &gt;= 19 ) |&gt; \n  group_by(Brand) |&gt; \n  summarize(avg_Rating = mean(Rating_Value, na.rm = TRUE),\n            number_Perfumes = n()) |&gt; \n  filter(number_Perfumes &gt;= 20) |&gt; \n  arrange(desc(avg_Rating)) |&gt; \n  head(n = 20)\n\nBefore you run this next line of code - what data do you expect to see?\nReading through the code above describe, as accurately as you can, what is and is not included in the brandRatingData object.\n\nbrandRatingData |&gt; head()\n\n# A tibble: 6 × 3\n  Brand                avg_Rating number_Perfumes\n  &lt;chr&gt;                     &lt;dbl&gt;           &lt;int&gt;\n1 Ensar Oud / Oriscent       8.38              93\n2 Nabeel                     8.27              22\n3 Clive Christian            8.14              45\n4 Roja Parfums               8.14             118\n5 Chanel                     8.11              83\n6 Annette Neuffer            8.10              20\n\n\nIs anything different between what you expected and/or described and what the object looks like?\n\n\n\n\n\n\nVariables in tidyverse\n\n\n\n\n\nIf you are new to the tidyverse, you have expected to see more columns, but group_by() and summarize() are determining what columns are generated and saved in the new object. The two filter() calls and the arrange() call control which rows are kept and the order we see them in.\n\n\n\nWith the brandRatingData object stored, we can now proceed to visualization.\n\n\n\n\n\nFamiliarise ourselves with the new objects\nWe need to import the transformed data objects and familiarise ourselves with them.\n\nperfume_brand_data &lt;- read.csv('https://raw.githubusercontent.com/GenomicsAotearoa/visualization_day/refs/heads/main/data/perfume_brand_data.csv')\n\nbrandRatingData &lt;- read.csv('https://raw.githubusercontent.com/GenomicsAotearoa/visualization_day/refs/heads/main/data/brandRatingData.csv')\n\n\nperfume_brand_data |&gt; head()\n\n# A tibble: 6 × 13\n  Number Name         Brand Release_Year Concentration Rating_Value Rating_Count\n  &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;        &lt;dbl&gt;\n1 150    : Contempor… Cliv…         2022 &lt;NA&gt;                   8.1           58\n2 150    : Timeless   Cliv…         2022 &lt;NA&gt;                   8.1           41\n3 1872   Acacia       Cliv…         2017 &lt;NA&gt;                   7.9           15\n4 1872   Basil        Cliv…         2018 &lt;NA&gt;                   7.8           48\n5 1872   Bergamot     Cliv…         2017 &lt;NA&gt;                   8.1           14\n6 1872   for Men      Cliv…         2001 &lt;NA&gt;                   8.2          407\n# ℹ 6 more variables: Main_Accords &lt;chr&gt;, Top_Notes &lt;chr&gt;, Middle_Notes &lt;chr&gt;,\n#   Base_Notes &lt;chr&gt;, Perfumers &lt;chr&gt;, URL &lt;chr&gt;\n\n\n\nbrandRatingData\n\n# A tibble: 20 × 3\n   Brand                          avg_Rating number_Perfumes\n   &lt;chr&gt;                               &lt;dbl&gt;           &lt;int&gt;\n 1 Ensar Oud / Oriscent                 8.38              93\n 2 Nabeel                               8.27              22\n 3 Clive Christian                      8.14              45\n 4 Roja Parfums                         8.14             118\n 5 Chanel                               8.11              83\n 6 Annette Neuffer                      8.10              20\n 7 Widian / AJ Arabia                   8.09              21\n 8 Aaron Terence Hughes                 8.09              61\n 9 Areej Le Doré                        8                 24\n10 Teone Reinthal Natural Perfume       7.92              25\n11 Stéphane Humbert Lucas               7.86              27\n12 Amouage                              7.86              92\n13 Jean Patou                           7.85              28\n14 Coty                                 7.84              25\n15 Swiss Arabian                        7.84              40\n16 Houbigant                            7.83              26\n17 Guerlain                             7.80             296\n18 Ormonde Jayne                        7.78              60\n19 Ex Nihilo                            7.77              40\n20 Afnan Perfumes                       7.77              29",
    "crumbs": [
      "Start to Finish Visualizing your Data"
    ]
  },
  {
    "objectID": "viz_start_to_finish.html#visualization-i",
    "href": "viz_start_to_finish.html#visualization-i",
    "title": "Start to Finish Visualizing your Data",
    "section": "Visualization I",
    "text": "Visualization I\nIn this example we will create a visualization of the top 20 perfume brands based on the average rating. The aim is to provide a novice with information about perfume brands so that they could pick a perfume with some confidence (e.g., if doing online shopping).\nIt is critically important to identify exactly what you are aiming to represent with your visualization. Biological data is often complex and, at least when publishing, space is often at a premium and figures tend to be information dense. You must consider the tension between including more information and readability.\nIn this workshop we are also aiming to create a figure that is visually appealing. We want to use elements of design such as line and colour theory etc., to capture and hold attention, whether this is during a live presentation or in a static format such as a journal.\n\nBuilding a visualization - sketch I\nTo start a visualization I will ‘sketch’ a figure to determine different ways to display the data. A boxplot is a great place to start since I know we are showing average ratings (even though we have so far calculated based on the mean and a boxplot uses medians).\n\nggplot(data = perfume_brand_data,\n       mapping = aes(x = Rating_Value, \n                      y = Brand)) +\n  geom_boxplot()\n\nWarning: Removed 563 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\nExercise: Why is this a poor visualization, and what can be done to improve it?\n\n\n\n\n\n\nCriticizing the boxplot\n\n\n\n\n\nLack of title, axis labels are ugly with underscore, no colour.\nUnsorted data is mentally tiring.\nOne-dimensional: does not show anything other than the median and summary.\nHow could this be improved?\nAdditional arguments in ggplot control title, set text size, etc.,.\nSort data and include other data to provide the reader with additional, useful information.\nSelect a clear, visually appealing colour scheme. Make sure colours are accessible, will work on other devices, text will be clear whether online or printed.\n\n\n\nFundamentally, I don’t think the boxplot works. While it does show the median and summary values, I don’t see a way to clearly show information like the number of perfumes (and, since that was a filtering criteria, I think it’s worth showing) or to make it visually interesting.\n\nExercise: Test out some other geoms and see if you can identify something that you like\n\n\n\nBuilding a visualization - sketch II\nNote: In reality you might sketch out a handful of different plots to find one that suits. Here we will jump ahead to something I think works.\nFirst, we will switch to using brandRatingData, where we have calculated the Brand average rating, and we will show the average with a geom_point function. You will see that while we lose some information switching from a boxplot to a geom_point() plot, this will allow us to include other information which may be more informative.\n\n\n\n\n\n\nThe importance of transforming your data\n\n\n\n\n\nWhile this workshop is primarily focused on the tools and code you need to visualize your data, the critical step above was calculating a new variable (the average rating value across all perfumes in a brand). While it may be possible to accurately visualize this information from the perfume_brand_data object, it would have been much more challenging.\n\n\n\nbrandRatingData is already formated to have Brands arranged by avg_Rating, but in case that wasn’t true, the code includes reorder() to sort by avg_Rating.\nThe labs() function is used to add a title, and clearer x and y axis labels.\n\nggplot(brandRatingData, \n       aes(\n        x = avg_Rating,\n        y = reorder(Brand, avg_Rating))) +\n  geom_point(aes(\n    size = number_Perfumes,\n    color = avg_Rating)) +\n  labs(\n    x = \"Brand rating\",\n    y = \"Brand\",\n    title = \"Mean perfume rating by brand\"\n  ) \n\n\n\n\n\n\n\n\nBy mapping average rating and the number of perfumes to colour and size (within the geom_point() function), we make the figure more visually interesting and more informative. For example, we can see that Ensar Oud/Oriscent has plenty of perfumes to choose from, while Nabeel has fewer. The plot overall looks ‘cleaner’ and less cluttered, yet actually includes more information.\nThis is the basis for a good visualization.\n\n\nRefining geom_point()\nHere we will bring in a handful of new arguments and functions. Remember that the main goal for you is not to memorise all of this information - this is a template for you to have available. Let’s look at some of the arguments below, and discuss the impact:\n\nggplot(brandRatingData, \n       aes(\n        x = avg_Rating,\n        y = reorder(Brand, avg_Rating))) +\n  geom_point(aes(\n    size = number_Perfumes,\n    color = avg_Rating)) +\n  scale_color_gradient(\n    low = \"lightblue\",\n    high = \"darkblue\",\n    name = \"Brand rating\") +\n  scale_size(\n    range = c(1, 10),\n    name = \"Number of perfumes\") +\n  labs(\n    x = \"Brand rating\",\n    y = \"Brand\",\n    title = \"Mean perfume rating by brand\"\n  ) +\n  theme_light() +\n  theme(\n    axis.text.y = element_text(size = 10),\n    axis.title = element_text(size = 12, face = \"bold\"),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = \"none\")\n\n\n\n\n\n\n\n\n\nscale_color_gradient() is a function to set colours through a given gradient. Useful if you have an overall brand or theme colour (e.g., University of Otago tends to use blue and gold, so I’ve chosen blue).\nscale_size() provides control over the size of the geom_point data points. We can set the upper/lower limits, and the ‘granularity’ (i.e., a difference between a large and a small value by having a greater range). Larger point sizes here (compared to our first plot) make the figure look less empty. Having points that overlap (Guerlain) may not be appropriate in all settings, but visually it makes the plot less ‘rigid’ and is more interesting. Note this function also sets the name to “Number of perfumes”.\nBy controlling both size and colour, the new plot focuses our attention, starting at the darker top right and letting our eyes move down and to the left (which is loosely our order of preference for brands). You can switch dark and light blue and re-build the plot, notice how much it impacts where our attention is drawn!\ntheme_light() sets the background space of the plot to white rather than a darker gray. Test this by changing it to theme_dark(). There are a set of available pre-made themes, and we can also control our own themes.\ntheme() is used to control almost everything else within the plot. Here we are setting three different sizes: the plot title, the axis names, and the axis text. Title and names are further differentiated by setting typeface to bold. This highlights an important feature of design: we naturally look for patterns and similarities, therefore we can use a point of difference to make something stand out.\nguides() is being used here to remove colour from the legend (color = “none”). Use size = “none” to remove the size legend. We have done this because while colour is being used to add visual interest, it isn’t really providing us with novel information (it is repeating the same information we get from looking at where the point sits on the x axis).\n\nUsing this information as a guide, you could make many fine scale tweaks to your plot, changing it to suit your specific data and situation.\n\n\nRefining our use of the data\nLooking at this plot, what can I do to enhance the visuals to be cleaner, more dynamic, more eye-catching, more aesthetic? Is there any other data I can include to improve the plot?\nCurrently, colour is not being fully utilised - it is mapped to the Brand average rating, which is already shown via the x axis. Thinking back to earlier themes on number of ratings and number of perfumes per brand, we could use colour to represent some of our data around the rating_Count variable.\nTo do this, we will re-build the brandRatingData object with an additional variable calculated by the summarize() function, calling it avg_rating_count and then map it to colour.\n\n# Add a new summarize output: the average rating count.\nbrandRatingData &lt;- parfumo_data_clean |&gt; \n  filter(Rating_Count &gt;= 19 ) |&gt; \n  group_by(Brand) |&gt; \n  summarize(avg_Rating = mean(Rating_Value, na.rm = TRUE),\n            number_Perfumes = n(),\n            avg_rating_count = mean(Rating_Count)) |&gt; \n  filter(number_Perfumes &gt;= 20) |&gt; \n  arrange(desc(avg_Rating)) |&gt; \n  head(n = 20)\n\nBefore we map avg_rating_count to colour we will consider accessibility. A colour palette, or visualization in general, is accessible if it is something that everyone can see. The most regularly discussed examples of accessibility include text size, a colour palette that can be seen by colour-blind individuals, and more recently whether the image is gentle or overwhelming.\n\n\n\n\n\n\nTip\n\n\n\nFor an example of ‘gentle vs overwhelming’, go to Tools, Global Options, Appearance, and in the Editor theme box, check “Gob”. I personally would not enjoy spending time looking at my RStudio console if the editor was stuck in “Gob”, and I prefer the much more gentle Idle Fingers theme.\n\n\nThe viridis R package is one example of a package that claims* to have colour-blind friendly colour palettes (* I’m not colour-blind and cannot confirm). In addition to mapping avg_rating_count to colour and using the scale_color_viridis() function with the “magma” argument, I’ve also tweaked some of the arguments in theme() to make the overall text package look cleaner and softer.\n\nlibrary(viridis)\n\nLoading required package: viridisLite\n\n# install.packages(\"ggtext\")\nlibrary(ggtext)\n\nggplot(brandRatingData, \n       aes(x = avg_Rating, y = reorder(Brand, avg_Rating))) +\n  geom_point(aes(size = number_Perfumes, color = avg_rating_count)) +\n  scale_color_viridis(option = \"magma\", direction = -1, name = \"Mean number of ratings\") +\n  scale_size(range = c(1, 10), name = \"Number of perfumes\") +\n  labs(\n    x = \"Brand rating\",\n    y = \"Brand\",\n    title = \"Top 20 Perfume Brands Ranked by Mean Rating\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_text(size = 10, color = \"gray40\"),\n    axis.title = element_text(size = 12, face = \"bold\", color = \"gray50\"),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5, color = \"gray50\"),\n    legend.title = element_text(size = 12, face = \"bold\", color = \"gray50\"), \n    legend.text = element_text(size = 10, color = \"gray40\")\n  ) \n\n\n\n\n\n\n\n\nThis version of the visualization is:\n\nmore visually interesting, with eye catching colours.\nsimultaneously softer and more gentle, more accessible overall.\ndisplays more data: importantly, we can now see that Ensar Oud/Oriscent has a low average number of ratings. With this in mind, I’d be more inclined to look at Roja Parfums or Chanel over Ensar Oud (and note how our colours help with this - Ensar Oud in the creamy yellow is not as eye catching as the darker Roja or Chanel).\nsuitable for conveying the basic information (Brand rating) at a glance. Even if a reader didn’t take note of the colours and sizes of the points, they would still get an idea of brand rating.",
    "crumbs": [
      "Start to Finish Visualizing your Data"
    ]
  },
  {
    "objectID": "viz_start_to_finish.html#final-exercise",
    "href": "viz_start_to_finish.html#final-exercise",
    "title": "Start to Finish Visualizing your Data",
    "section": "Final exercise",
    "text": "Final exercise\nUse the information we have discussed, and the lessons from earlier, to trial out some different geoms and visualization styles. See what code you can apply (e.g., changing size/colour) with other geoms.",
    "crumbs": [
      "Start to Finish Visualizing your Data"
    ]
  },
  {
    "objectID": "viz_start_to_finish.html#summary",
    "href": "viz_start_to_finish.html#summary",
    "title": "Start to Finish Visualizing your Data",
    "section": "Summary",
    "text": "Summary\nBefore we can visualize our data we may have to transform it in some way or another. We may have to make difficult choices around thresholds of significance or what data to include or remove.\nCareful choice of geoms and mapping allow us to show data on more than just two or three axes.\nThe goal isn’t just to show the data. It’s to excite the audience, to capture and retain attention in an extremely noisy world.",
    "crumbs": [
      "Start to Finish Visualizing your Data"
    ]
  },
  {
    "objectID": "viz_1_ggplot.html",
    "href": "viz_1_ggplot.html",
    "title": "Visualization with ggplot2",
    "section": "",
    "text": "In this section we will cover the basic format of ggplot functions. We will note the format and key arguments. This code will be repeated throughout the workshop, and provided frequently as a template: therefore, the goal is not to memorise, but to be able to recognise and interpret while we build in complexity throughout the day.",
    "crumbs": [
      "Visualization with ggplot2"
    ]
  },
  {
    "objectID": "viz_1_ggplot.html#the-ggplot-format",
    "href": "viz_1_ggplot.html#the-ggplot-format",
    "title": "Visualization with ggplot2",
    "section": "The ggplot format",
    "text": "The ggplot format\nMany modern R workshops include a section on the Grammer of Graphics, or the ggplot2 function, and there is no shortage of detailed workshops and tutorials available if you want more detailed explanations of the basics.\nggplot2 is a way to create visualisations within the tidyverse. Once you recognise the template it will become quick and easy to create a variety of plots with different data types with minimal extra work.\nThe format for the ggplot2 template is as follows:\n\nSpecify the data\nMap variables e.g., map count data in column a to the x axis\nCreate the plot\n\n\n# The code to create a plot, using the template above:\nggplot(data = penData,\n       mapping = aes(x = species,\n                     y = bill_length_mm)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n# Here we have specified that the data will come from the penData object\n# Data from the species variable will be on the x axis\n# Data from the bill_length_mm variable will be on the y axis\n# The type of plot is a boxplot  \n\nSome things to note about the format:\n\nIndentations are important. We use new lines and tabs to keep the code organised. Generally you’ll want to specify only one thing per line (e.g., data, x axis and y axis get there own lines).\nThe ggplot formula is a slight break away from the use of pipes.\nThere are actually two separate functions here: the ggplot() function, which is used to specify the data and map the variables, and the geom_boxplot() function which is used to create the actual plot. Because we want these two functions to work together, at the very end of the ggplot() function, we have added a “+” symbol. RStudio interprets this to mean “The ggplot function has finished, but it must be interpreted in the context of the next function”.\ngeom_boxplot() is the function for making boxplots. To make a bar plot we would use geom_bar(), to make a scatter plot we use geom_point() etc.,. Type geom_ in the console and scroll the dropdown menu to see the different geom types - there are plenty!",
    "crumbs": [
      "Visualization with ggplot2"
    ]
  },
  {
    "objectID": "viz_1_ggplot.html#extending-the-ggplot-format",
    "href": "viz_1_ggplot.html#extending-the-ggplot-format",
    "title": "Visualization with ggplot2",
    "section": "Extending the ggplot format",
    "text": "Extending the ggplot format\nWe can extend the plot in three easy ways:\n\nMapping additional data to variables (e.g., species or sex to variables such as colour and shape)\nSupplying additional arguments to the geom function (e.g., changing the size, colour or opacity of dots)\nAdd new functions that control features such as the title or axes labels.\n\nIn all cases, these new arguments and functions follow the template as outlined above. We will continue to use indentations and new lines to keep our code organised and tidy.\nThis example creates a more complex plot, but these new arguments and functions follow the template as outlined above. We continue to use indentations and new lines to keep our code organised and tidy.\n\n# Load the viridis library, which supplies a colour scheme that is\n# sensitive to colour-blindness.\n\n# install.packages(\"viridis\")\nlibrary(viridis)\n\nLoading required package: viridisLite\n\n# New arguments map species to the colour variable, so that each point is coloured\n# by species.\n# Sex is mapped to shape. Note that shape can only take discrete variables.\nggplot(data = penData,\n       mapping = aes(x = bill_length_mm,\n                     y = flipper_length_mm,\n                     colour = species,\n                     shape = sex)) +\n  geom_point(size = 2, alpha = 0.6) + # alpha is transparency of the points\n  scale_colour_viridis(discrete = TRUE) +\n  labs(x = \"Bill depth (mm)\",\n       y = \"Flipper length (mm)\") +\n  ggtitle(\"Bill depth vs flipper length\")",
    "crumbs": [
      "Visualization with ggplot2"
    ]
  },
  {
    "objectID": "viz_1_ggplot.html#summary",
    "href": "viz_1_ggplot.html#summary",
    "title": "Visualization with ggplot2",
    "section": "Summary",
    "text": "Summary\nThe ggplot2 template has a simple layout that we will build on throughout the day. Do not worry about memorising all the additional functions or arguments, and expect to use templates as references while you are learning. There will be plenty of examples of code you can refer to throughout this set of material.\nA core idea for today is to keep our code as tidy and as well annotated as possible. Endeavor to use comments, section headings, indents and a cohesive format throughout the day.",
    "crumbs": [
      "Visualization with ggplot2"
    ]
  },
  {
    "objectID": "blog/first-post/index.html",
    "href": "blog/first-post/index.html",
    "title": "First Post",
    "section": "",
    "text": "Sed risus ultricies tristique nulla aliquet. Neque volutpat ac tincidunt vitae semper quis lectus nulla.\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Enim sed faucibus turpis in eu mi bibendum neque. Ac orci phasellus egestas tellus rutrum tellus pellentesque eu. Velit sed ullamcorper morbi tincidunt ornare massa. Sagittis id consectetur purus ut faucibus pulvinar elementum integer. Tincidunt nunc pulvinar sapien et ligula ullamcorper malesuada proin libero. Lobortis feugiat vivamus at augue eget arcu. Aliquam ut porttitor leo a diam sollicitudin tempor id eu. Mauris a diam maecenas sed enim ut sem viverra aliquet. Enim ut tellus elementum sagittis vitae et leo duis. Molestie at elementum eu facilisis sed odio morbi quis commodo. Sapien pellentesque habitant morbi tristique senectus. Quam vulputate dignissim suspendisse in est. Nulla pellentesque dignissim enim sit amet venenatis urna cursus eget.\nVelit aliquet sagittis id consectetur purus ut faucibus pulvinar elementum. Viverra mauris in aliquam sem fringilla ut morbi tincidunt augue. Tortor at auctor urna nunc id. Sit amet consectetur adipiscing elit duis tristique sollicitudin. Aliquet nibh praesent tristique magna sit amet purus. Tristique senectus et netus et malesuada fames ac turpis. Hac habitasse platea dictumst quisque. Auctor neque vitae tempus quam pellentesque nec nam aliquam. Ultrices tincidunt arcu non sodales neque sodales ut etiam. Iaculis at erat pellentesque adipiscing. Cras tincidunt lobortis feugiat vivamus. Nisi est sit amet facilisis magna etiam. Pharetra pharetra massa massa ultricies mi quis hendrerit. Vitae sapien pellentesque habitant morbi tristique senectus. Ornare aenean euismod elementum nisi quis eleifend quam adipiscing vitae."
  },
  {
    "objectID": "viz_2_exploratoryAnalysis.html",
    "href": "viz_2_exploratoryAnalysis.html",
    "title": "Visualization for ourselves - Exploratory Analysis",
    "section": "",
    "text": "Visualization is most often thought of as something we do for other people, to share some piece of knowledge we have. But equally important is the role visualization can play in us learning something about our data. Visualization can reveal trends or associations that we would otherwise struggle to identify in observations stored in rows and columns.\nIn this section we will perform some quick data visualizations that we can use for our own observations. The output of this section does not need to be overly attractive, but it does need to be clear and should ‘standalone’ (i.e., should speak for itself, without the need of accompanying notes).\nThe aims for this section are to give you the opportunity to practice some of the most important code used in visualizations and to force you to think about visualization: what types of figures work well for certain data, how to display the information you want to convey.\nWe will examine one type of graph to represent four different types of data: distribution, correlation, parts of a whole, and evolution (change over time).",
    "crumbs": [
      "Visualization for ourselves - Exploratory Analysis"
    ]
  },
  {
    "objectID": "viz_2_exploratoryAnalysis.html#distribution",
    "href": "viz_2_exploratoryAnalysis.html#distribution",
    "title": "Visualization for ourselves - Exploratory Analysis",
    "section": "Distribution",
    "text": "Distribution\nDistribution plots are a good place to start any data analysis, because many statistical tests rely on the assumption of a normal distribution. We may also make our own assumptions about how data is similar or different between different groups. The classic argument for distribution plots is the “datasaurus dozen” - a set of 13 datasets with identical summary statistics (same mean of x, mean of y, Std Dev etc.,) but when plotted reveal funny patterns (including points in the shape of a dinosaur).\nDistribution matters!\n\nBeeswarm plots with ggbeeswarm\nHere we will use a type of plot, called a beeswarm plot, to highlight the power of visualization.\nBeeswarm plots can be made using a standalone package (install.packages(“beeswarm”)) or using a package that extends ggplot2 (install.packages(“ggbeeswarm”)). We will use the later, since it let’s us work within the ggplot format we have looked at already.\n\nPackages and setup\nInstall the ggbeeswarm package, then load the ggbeeswarm and palmerpenguins packages.\n\n#install.packages(\"ggbeeswarm\")\n\nlibrary(ggbeeswarm)\n\nLoading required package: ggplot2\n\nlibrary(palmerpenguins)\nlibrary(tidyverse) # We will use tidyverse later\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.4     ✔ tibble    3.2.1\n✔ purrr     1.0.4     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nBeeswarm plots\nLike with the geom_boxplot() we looked at in the previous episode, we need to specify where the data comes from and how the data is mapped. Beeswarm plots are simple and we will initially only need to specify the x and y axis data.\n\nggplot(data = penguins,\n        mapping = aes(x = species,\n                      y = flipper_length_mm)) +\n  geom_beeswarm()\n\n\n\n\n\n\n\n\nNote: When running this code, ggplot gives us a warning that rows were removed due to missing values. I’ve added a hidden line of code to suppress these warnings.\nWith this plain plot we can see what beeswarm is doing - when points on the y axis share a value they are moved horizontally so that overlapping points can be identified. This concept is commonly referred to as “jitter”, and “jitter plots” are a class of chart. Beeswarm plots are an implementation of jitter plots that aim to be compact with as little as possible overlap of points. This gives us the ability to see density through the width of the plot, similar to how a violin plot (geom_violin) works.\nDistribution plots are a good opportunity to introduce two new parameters we can adjust in ggplot: size and alpha.\n\nSize is simply the size of the data point. We can assign a single value for all points, or can assign a continuous variable.\nAlpha is the level of transparency. When data points are too large (from the size argument), or too numerous, the points can overlap one another (a problem called “overplotting”). This is particularly problematic outside of beeswarm. When we set alpha to something less than 1, the points become somewhat transparent. Therefore, if two points overlap one another, it is clear from the change in colour (as two transparent but overlapping points are darker than a single point).\n\n\nggplot(data = penguins,\n        mapping = aes(x = species,\n                      y = flipper_length_mm,\n                      colour = species)) +\n  geom_beeswarm(size = 2, alpha = 0.7)\n\n\n\n\n\n\n\n\n(I’d argue that increasing the size and decreasing the alpha has decreased the quality of our plot, but it’s just for the purpose of demonstration).\n\n\n\ngeom_quasirandom and dangerous distributions\ngeom_quasirandom() is a function very similar to geom_beeswarm(), but instead of strictly moving points, they are quasi-randomly moved.\n\nExercise: Compare the plot below with the plot above. What is different between them?\n\n\n\n\n\n\nDangers of distributions\n\n\n\n\n\nPersonally, I find these two plots give very different impressions of the data. In the plot below, the wide shape (particularly for the Chinstrap species) is reminiscent of a normal distribution and it subconciously reminds me of that. Looking at the plot above, I don’t immediately think of a normal distribution. It’s easy to imagine a situation where the wrong choice of plot and a lapse in concentration could give the wrong impression.\nYour choice of visualization can have a strong impact on what people see.\n\n\n\n\nggplot(data = penguins, \n        mapping = aes(x = species, \n                      y = flipper_length_mm,\n                      colour = species)) +\n  geom_quasirandom(size = 2, \n                    alpha = 0.7)\n\n\n\n\n\n\n\n\n\n\nTheme and labels\nWe can add two other functions to make this plot much easier to read. First, the theme_minimal() function is a quick way to set a global theme to the plot and make it less cluttered. The second function is labs(), which we use to add a title and x and y axes labels. We will cover themes more later, but adding theme_minimal() to every plot we do today will keep them looking tidy and coherent.\n\nggplot(data = penguins, \n        mapping = aes(x = species,\n                      y = flipper_length_mm,\n                      color = species)) +\n  geom_quasirandom(size = 2, alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"Flipper Length Distribution by Species\",\n       x = \"Species\",\n       y = \"Flipper Length (mm)\")",
    "crumbs": [
      "Visualization for ourselves - Exploratory Analysis"
    ]
  },
  {
    "objectID": "viz_2_exploratoryAnalysis.html#correlation",
    "href": "viz_2_exploratoryAnalysis.html#correlation",
    "title": "Visualization for ourselves - Exploratory Analysis",
    "section": "Correlation",
    "text": "Correlation\n\nHeatmaps with geom_tile()\nHeatmaps can be used to visualize values across two dimensions using colour. In a biological context this could be gene expression values across a range of genes for a set of samples. This is often combined with some type of clustering (e.g., unsupervised hierarchical clustering with k-means) to identify patterns of gene expression which differ between sample groups.\nHere we will use geom_tile() to create a particular type of heatmap called a “calendar heatmap”, which will display a value on a given day for a selected time period. The example below is captured from github, which display the number of commits (saves) a user made across the year.\n\n\n\nGithub annual contributions\n\n\nThat’s a lot of commits!\n\nCalendar heatmap\nWe will create a simple calendar heatmap to demonstrate the concept. Remember, the goal here is to expose you to different types of plots and for you to practice using ggplot2.\n\n\nGenerating example data\nFirst, we will generate some example data. Copy and paste this code, because it is not central to our understanding:\n\nset.seed(0982)\nmonth_data &lt;- tibble(\n  date = seq(as.Date(\"2024-12-01\"), \n              as.Date(\"2024-12-31\"), \n              by = \"day\"),\n  count = sample(1:20, 31, replace = TRUE)  # Random counts per day\n) |&gt;\n  mutate(\n    wday = wday(date, label = TRUE, abbr = TRUE),  # Day of the week (Sun-Sat)\n    week = (day(date) - 1) %/% 7 + 1  # Week number (1 to 5)\n  )\n\nmonth_data |&gt; head()  \n\n# A tibble: 6 × 4\n  date       count wday   week\n  &lt;date&gt;     &lt;int&gt; &lt;ord&gt; &lt;dbl&gt;\n1 2024-12-01     2 Sun       1\n2 2024-12-02     3 Mon       1\n3 2024-12-03    18 Tue       1\n4 2024-12-04    19 Wed       1\n5 2024-12-05     8 Thu       1\n6 2024-12-06     2 Fri       1\n\n\n\n\ngeom_tile()\nNow we will create the plot with geom_tile(). Like with the geom_boxplot() we looked at in the previous episode, we need to specify where the data comes from and how the data is mapped. With geom_tile() we will always need to use aes() to specify the x and y, and we will also specify the variable that is used to determine the colour of each tile, which we do with “fill =”. In our example we will map the count variable to fill.\n\nggplot(data = month_data,\n       mapping = aes(x = week, \n                      y = wday, \n                      fill = count)) +\n  geom_tile()\n\n\n\n\n\n\n\n\nThis is a good starting point in that we have functionally sketched the data into the plot type. However, we can see that it the layout does not match our expectations for a calendar: it reads the days in reverse (Saturday, Friday, Thursday etc.,), and has the week on the x axis. It is also missing a title, and axes labels, and it doesn’t have particularly high contrast between high and low values.\nTo correct these issues we will:\n\nSwap the x and y axis with the aes() function.\nUse the scale_y_reverse() function to reverse the Y-axis (which will now be weeks) so that the last value (week 5) is placed at the bottom.\nAdd the scale_fill_viridis_c() function to change the colour of the fill. The “scale_[argument]_viridis” set of functions are used to select a colour blind-friendly pallette with high contrast.\nUse the labs() function to add title, x and y axis labels, and rename the fill (legend) variable.\nAdditionally, we will add colour = “white” as an argument for geom_tile(), which will result in a fine white grid separating our tiles, and add the theme_minimal() function to reduce the visual clutter of the grey background in the plot.\n\n\n\nExercise: Use the code block and tips above to create an improved version of the calendar heatmap.\nYou do not need to do all of these changes, but try them out if you can! Remember to add a “+” at the end of each line when adding new functions.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nggplot(data = month_data,\n       mapping = aes(x = wday, \n                      y = week, \n                      fill = count)) +\n  geom_tile(colour = \"white\") +\n  scale_fill_viridis_c() +\n  scale_y_reverse() +\n  theme_minimal() +\n  labs(title = \"December 2024 Calendar Heatmap\", \n        x = \"Week\", \n        y = \"Day of the Week\", \n        fill = \"Count\")",
    "crumbs": [
      "Visualization for ourselves - Exploratory Analysis"
    ]
  },
  {
    "objectID": "viz_2_exploratoryAnalysis.html#parts-of-a-whole",
    "href": "viz_2_exploratoryAnalysis.html#parts-of-a-whole",
    "title": "Visualization for ourselves - Exploratory Analysis",
    "section": "Parts of a whole",
    "text": "Parts of a whole\nIn this section we will take a break from executing code and look at some examples. This is an opportunity to introduce you to the excellent resource that is the R-graph-gallery. We will scroll down to the “Parts of a whole” section, and walk through how this resource not only shows you different types of plots you can create, but provides you with code and reproducible examples.\nYou can jump straight to our first example, the waffle chart, here. The R-graph-gallery gives an overview of the type of chart and highlights some of the key ways to get started. In the case of the waffle chart we can either use a dedicated package or explore how to build a waffle chart with ggplot2 syntax. Finally, there are three examples of high quality waffle charts - click on any of these three charts to get a detailed explanation of the code used to create the chart and an example dataset. This example by Muhammad Azhar is my favourite, and they have all the code required to help you build your own version.\nBack on the main page we can see that this same level of information exists for other chart types too. Take two minutes to pick one of the Parts of a Whole plots and explore some of the information about this type of chart.",
    "crumbs": [
      "Visualization for ourselves - Exploratory Analysis"
    ]
  },
  {
    "objectID": "viz_2_exploratoryAnalysis.html#evolution-over-time",
    "href": "viz_2_exploratoryAnalysis.html#evolution-over-time",
    "title": "Visualization for ourselves - Exploratory Analysis",
    "section": "Evolution (over time)",
    "text": "Evolution (over time)\nHere we will look at three variations of the line plot, which is used to show changes in a variable over time.\n\nGenerate a dataset\n\nset.seed(02) # Please set this seed for this example\n\n# Create stock price data for three different stocks\ndates &lt;- seq.Date(from = as.Date(\"2024-01-01\"), \n                  by = \"week\", \n                  length.out = 200)\n\nstocks &lt;- tibble(\n  date = rep(dates, times = 3),\n  stock = rep(c(\"Stock A\", \"Stock B\", \"Stock C\"), each = length(dates)),\n  price = c(\n    100 + cumsum(rnorm(length(dates), mean = 0, sd = 5)),  # Stock A fluctuates around 100\n    80 + cumsum(rnorm(length(dates), mean = 0, sd = 4)),   # Stock B fluctuates around 80\n    120 + cumsum(rnorm(length(dates), mean = 0, sd = 6))   # Stock C fluctuates around 120\n  )\n)\n\n\n\n\n\n\n\nWhat is set.seed?\n\n\n\n\n\nWhat is set.seed? set.seed() is a function that feeds into any function that will randomly select or generate values. If we all use set.seed with the same number value, we will all get the same results, even though we are about to ‘randomly’ simulate some data. In future you can use any numeric value for set.seed(), so long as you keep it consistent within experiments.\n\n\n\n\n\nLine chart with geom_line()\nThe simplest plot for change over time is geom_line().\n\nggplot(stocks %&gt;% filter(stock == \"Stock C\"), \n        aes(x = date, \n            y = price)) +\n  geom_line() +\n  labs(title = \"Stock C Price Over Time\", \n        x = \"Date\", \n        y = \"Price\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nVisual interest with geom_area()\nThe geom_area() function uses the same principal but also includes a filled or shaded area below the line. This is the same data, but due to the shading makes the whole plot look less empty.\n\nggplot(stocks %&gt;% filter(stock == \"Stock B\"), \n        aes(x = date, y = price)) +\n  geom_area(fill = \"black\", \n            alpha = 0.4) +\n  geom_line(color = \"black\", \n            size = 0.5) +  # Keeps the line for clarity\n  labs(title = \"Stock B Price Over Time (Area Chart)\", \n        x = \"Date\", \n        y = \"Price\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nThe choice of geom_line() or geom_area() is purely aesthetic.\n\n\nStacked area chart\nThe stacked area chart can be used to create a strong visual impact, especially if the correct colours are chosen. Here’s an example from Cédric Scherer:\nAppearance of the popular X-Men\n\nggplot(stocks, aes(x = date, \n                    y = price, \n                    fill = stock)) +\n  geom_area(alpha = 0.6) +  # Fill areas with transparency\n  labs(title = \"Stacked Area Chart of Stock Prices Over Time\",\n       x = \"Date\", \n       y = \"Price\", \n       fill = \"Stock\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nOn the dubious rise of our stocks…\n\n\n\n\n\n\n…and the flaw with stacked area charts\n\n\n\n\n\nLooking at the stacked area plot you would be forgiven for thinking all three stocks have increased over time. Looking more closely you will notice that Stock A has not increased in value and is almost exactly where it started.\nBecause stacked area plots can be misleading, they are not recommended as a type of visualization.\nYou can check the actual change in value of Stock A by repeating the code for the line or single variable area plot, changing the code to filter(stock == “Stock A”) (and updating the figure title of course).\nAn alternative to stacked area plots is the use of facet, which will allow us to visualize multiple stocks at once by creating individual panels.",
    "crumbs": [
      "Visualization for ourselves - Exploratory Analysis"
    ]
  },
  {
    "objectID": "viz_2_exploratoryAnalysis.html#summary",
    "href": "viz_2_exploratoryAnalysis.html#summary",
    "title": "Visualization for ourselves - Exploratory Analysis",
    "section": "Summary",
    "text": "Summary\nThe two key messages from this episode are:\n\nThere are lots of different chart types available to you, and they mostly follow the same template with slight variations in the geom_() function and the arguments supplied.\nHow you visualize your data can have an impact on what other people see. The human brain is amazing, but even on our best days we make assumptions, or can be distracted, and reach the wrong conclusion. Strive to make your data as clear and transparent as possible.",
    "crumbs": [
      "Visualization for ourselves - Exploratory Analysis"
    ]
  },
  {
    "objectID": "viz_3_exemplarVisualizations.html",
    "href": "viz_3_exemplarVisualizations.html",
    "title": "Exemplar visualizations - what far can we take it?",
    "section": "",
    "text": "We’ve seen how to make ‘quick and dirty’ plots for ourselves. Now we will give you a glimpse of just how far you can take your visualizations. For this section we are looking at both code complexity and fundamental elements of design.\nThe aim of this section is to give you a set of resources that you can use to find examples and inspiration for your own work, as well as starting to look at elements of design.\nHere we will provide guidelines on design and visualization while highlighting key examples from the scientific community. The good news is that there is an avid visualization community online - check out the 30 day chart challenge or the datavisualization hashtag on LinkedIn as a starting point.",
    "crumbs": [
      "Exemplar visualizations - what far can we take it?"
    ]
  },
  {
    "objectID": "viz_3_exemplarVisualizations.html#from-the-community",
    "href": "viz_3_exemplarVisualizations.html#from-the-community",
    "title": "Exemplar visualizations - what far can we take it?",
    "section": "From the community",
    "text": "From the community\n\nCédric Scherer\nCédric Scherer’s Data Visualization Gallery\nWhen looking through this gallery, note that while some plots are very complex, they are built from basic elements. Each visualization is carefully designed: colour palettes are interesting but not overwhelming. There is adequate spacing between plot space and titles, axes, and text. Paler text (grey instead of black) makes overall images gentler. Cédric is a master of design as well as code, and all these visualizations demonstrate careful thought and planning.\n\nThis image comes from Cédric Scherer, and shows the relationship between the number of US Views and the number of votes on IMDb (a site that hosts voting on tv/movie quality). It looks like US viewers are heavy contributors in the IMDb voting system!\nKey things to note about this visualization:\n\nUse of a lower alpha value is a functional way to represent density. In addition to the wider scatter plot, there’s a clear pattern of higher density which probably contributes heavily to the overall trend line.\nThe colour scheme is eye-catching but not abrasive. In design/art this is called a monocolour (a single colour with white or black to modify the colour) - it’s highly cohesive and will generally be considered aesthetically pleasing. Note how the text uses the same colour, and the bolded text is also a colour found on the plot itself. The plot is probably using theme_minimal().\nThere is a hierarchy of black, with the title in black, the sub-heading in dark grey, the text below the chart in two separate light greys. This draws the eye to the top and guides the eye downwards. The two axis labels break from this, but only slightly, so that they stand out subtly - you can see them but they don’t distract you.\nText on the plot: this is not too common in scientific journal articles, but is a useful tool for science communication to a wider audience.\n\n\n\nThe gallery of Dr Nicola Rennie\nDr Nicola Rennie is a data visualization specialist and active community leader (note, the link can be used to find a list of Dr Rennie’s publications, and some of those offer in-depth run-throughs of visualization practices). Nicola has created a Shiny app to function as a gallery for all their 30daychartchallenge entries. Not only is this a stunning gallery with a wide array of chart types and top quality visualization ideas, but all of the code is available on github (to navigate to the R code: click on a given year (e.g., 2022), then click on scripts, then open the file for a given plot).\n\nExcerise: Take a few minutes to explore Nicola’s gallery and make some observations about the charts. What do you like? Do you see anything you would do differently? Can you identify a use for one of these charts for visualizing your own data?\n\n\n\n\n\n\nObservations from the gallery\n\n\n\n\n\n2024, Day 13 (Family) - very simple plot, the yellow-green is quite an interesting colour. Nice use of highlighting the female boxes in that colour and also having the text box share that colour. Good demonstration of a plot I would have been tempted to make in powerpoint.\n2024, Day 1 (Part to Whole) - I love this colour scheme, but it’s not easy to tell what the three pink bars represent, and I wonder if this chart would have worked with median income on the Y axis (since we say “higher” income, having income on a vertical axis makes sense).\n2024, Day 8 (Circular) - I understand that this colour scheme is picked to fit with the theme of Marvel comics, and I can see how it works with the original “X-MEN” logo, but I personally steer away from these bright yellows and reds, and while I find the yellow on dark blue text has high contrast, I wouldn’t describe it as easy to read.\n2022, Day 1 (Part to whole) - This yellow on purple has slightly lower contrast but, at least to my eyes, is much nicer and easier to read.\n\n\n\n\n\n\nIsaac Arrayo\nIsaac Arrayo: towards data science\nIsaac Arrayo gives us a breakdown of four plots from #tidytuesday (a community effort where people are invited to create visualizations of a provided dataset). We see four diverse plot types, the rationale for each plot, and how these plots might be applied to a more ‘standard’ dataset (i.e., an in-real-life example).",
    "crumbs": [
      "Exemplar visualizations - what far can we take it?"
    ]
  },
  {
    "objectID": "viz_3_exemplarVisualizations.html#resources-and-showcase",
    "href": "viz_3_exemplarVisualizations.html#resources-and-showcase",
    "title": "Exemplar visualizations - what far can we take it?",
    "section": "Resources and Showcase",
    "text": "Resources and Showcase\nA collection of resources - if you have any recommendations, reach out to me on LinkedIn or by opening an issue on github and I’ll add them here.\n\nGalleries, books, workshops\n\nThe r-graph-gallery\nThe r-graph-gallery\nThe R graph gallery has a section on ggplot and it highlights some of the exciting features: custom fonts, pre-built themes, interactive plots - there’s a lot going on.\n\n\nfrom Data to Viz\nI highly recommend from Data to Viz as a resource for thinking about your data and what type of chart might be appropriate.\n\n\n\nfrom Data to Viz decision tree\n\n\nFirst, it shows a decision tree for each of the different data types (numeric, categoric etc.,). Within the data types you can look at plots for different subcategories (single numeric data, two numeric variables etc.,) and there is a “story” article for each subcategory that covers an example and explanations.\n\n\n\nfrom Data to Viz plot information\n\n\nYou can also click on a given plot type in the decision tree to open up a panel and view information about the plot type. Each plot also has a dedicated page that gives a full breakdown of the plot, explaining the required variables and some beautiful examples.\n\n\nR for Data Science, 2E\nThe R for Data Science, 2E book (available free, online) is the go to resource for anyone learning the tidyverse, and it has a great section on ggplot2!",
    "crumbs": [
      "Exemplar visualizations - what far can we take it?"
    ]
  },
  {
    "objectID": "viz_3_exemplarVisualizations.html#summary",
    "href": "viz_3_exemplarVisualizations.html#summary",
    "title": "Exemplar visualizations - what far can we take it?",
    "section": "Summary",
    "text": "Summary\nElements of design, or the ability to create attention-capturing visuals, is not widely taught (in my experience) in the science sector, but there is a range of useful material and dialogue on visualization available to you. Remember that everyone will have their own personal preference, and it’s important to balance style with clarity. Simplicity is always going to be a powerful factor, and within a single document having a cohesive theme is key.",
    "crumbs": [
      "Exemplar visualizations - what far can we take it?"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "From Start to Finish: Visualizing Your Data",
    "section": "",
    "text": "Good visualization is about more than just writing the correct code. Visualization is communication, and good communication requires thinking about how we as the scientist/data analyst/creator convey our message to the reader. This one-day workshop will provide you with a toolbox of skills and code examples to help you create clear, visually interesting visualizations which accurately communicate a message."
  },
  {
    "objectID": "index.html#this-workshop-has-five-sections",
    "href": "index.html#this-workshop-has-five-sections",
    "title": "From Start to Finish: Visualizing Your Data",
    "section": "This workshop has five sections",
    "text": "This workshop has five sections\nThe basic code template for ggplot.\nVisualization for our selves (exploratory analysis).\nExamples of good visualization (with code).\nIteratively building a figure.\nVisualization from start to finish."
  },
  {
    "objectID": "index.html#why-are-we-doing-this",
    "href": "index.html#why-are-we-doing-this",
    "title": "From Start to Finish: Visualizing Your Data",
    "section": "Why are we doing this?",
    "text": "Why are we doing this?\nBecause some plots look like this (Credit to Dr. Joseph Guhlin for this presentation)."
  },
  {
    "objectID": "blog/third-post/index.html",
    "href": "blog/third-post/index.html",
    "title": "Third Blog Post",
    "section": "",
    "text": "The source for any page in your website could also be a Jupyter Notebook. This one is third-post/index.ipynb.\nHere’s an example I borrowed from the Seaborn docs:\n\nimport seaborn as sns\n\nsns.set_theme(style=\"whitegrid\")\n\n# Load the diamonds dataset\ndiamonds = sns.load_dataset(\"diamonds\")\n\n# Plot the distribution of clarity ratings, conditional on carat\nsns.displot(\n    data=diamonds,\n    x=\"carat\", hue=\"cut\",\n    kind=\"kde\", height=4, aspect=1.5,\n    multiple=\"fill\", clip=(0, None),\n    palette=\"ch:rot=-.25,hue=1,light=.75\",   \n)"
  },
  {
    "objectID": "blog/second-post/index.html",
    "href": "blog/second-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Quis imperdiet massa tincidunt nunc pulvinar sapien et ligula. Amet cursus sit amet dictum sit amet. Eget duis at tellus at urna condimentum. Convallis aenean et tortor at risus viverra. Tincidunt ornare massa eget egestas purus viverra accumsan. Et malesuada fames ac turpis egestas. At imperdiet dui accumsan sit amet. Ut ornare lectus sit amet est placerat. Enim nulla aliquet porttitor lacus luctus accumsan tortor posuere. Duis ultricies lacus sed turpis tincidunt id aliquet risus. Mattis enim ut tellus elementum sagittis. Dui id ornare arcu odio ut. Natoque penatibus et magnis dis. Libero justo laoreet sit amet cursus sit. Sed faucibus turpis in eu. Tempus iaculis urna id volutpat lacus laoreet.\nPhasellus vestibulum lorem sed risus. Eget felis eget nunc lobortis mattis. Sit amet aliquam id diam maecenas ultricies. Egestas maecenas pharetra convallis posuere morbi. Etiam erat velit scelerisque in dictum non consectetur a erat. Cras fermentum odio eu feugiat pretium nibh ipsum consequat. Viverra accumsan in nisl nisi scelerisque. Et netus et malesuada fames ac. Amet tellus cras adipiscing enim eu turpis egestas pretium aenean. Eget lorem dolor sed viverra ipsum nunc aliquet. Ultrices dui sapien eget mi proin sed libero enim sed. Ultricies mi eget mauris pharetra et ultrices neque. Ipsum suspendisse ultrices gravida dictum. A arcu cursus vitae congue mauris rhoncus aenean vel. Gravida arcu ac tortor dignissim convallis. Nulla posuere sollicitudin aliquam ultrices."
  },
  {
    "objectID": "viz_4_building_better.html",
    "href": "viz_4_building_better.html",
    "title": "Building Better Plots",
    "section": "",
    "text": "The purpose of this episode is to:\nand\nThis episode follows the outline produced by the very skilled data visualizer Cédric Scherer, modified to work with some example data and updated with my own thoughts and opinions. Cédric’s original tutorial is well worth looking at, as they are a real expert in the field of visualization.",
    "crumbs": [
      "Building Better Plots"
    ]
  },
  {
    "objectID": "viz_4_building_better.html#the-data",
    "href": "viz_4_building_better.html#the-data",
    "title": "Building Better Plots",
    "section": "The data",
    "text": "The data\nThe example dataset we are using for this session comes from Wilderlab, a company based in Aotearoa who provide eDNA monitoring and testing services (primarily from water samples). When a client orders DNA testing through Wilderlab they have the option of making their results public as a way to “provide a useful resource for scientists, conservationists, educators, and anyone else with an interest in Aotearoa’s biodiversity, water quality and biosecurity”. We will access a small subset of the data which I have accessed and curated for the workshop. It is currently stored on github as a .tsv file.\neDNA sequences are assigned a taxonomy, and when multiple sequences map to the same taxonomic unit (in our case to the level of the same genus), those additional sequences contribute to the variable called Count. I selected the 10 Groups (a classification level, e.g., Birds, Fish, Diatoms, Plants) with the highest number of sequences (that is, the groups with the most species diversity), randomly selected 500 of these sequences for each group, and saved the data to a file. We can now look at Count (the abundance) of these sequences and see if this varies across Groups.\nExercise: Based on what you know about eDNA, predict and rank the 10 groups based on the average level of eDNA abundance. The 10 groups are:\nBirds, Ciliates, Diatoms, Fish, Insects, Mammals, Molluscs, Other, Plants, Worms\n\n\n\n\n\n\nA contrived example\n\n\n\n\n\nFor the sake of honesty, if the above explanation of how I created our data subset sounds arbitrary - it is. Originally I wanted an example look at the relationship between Group diversity (number of species) and Count (abundance). I hypothesised that a Group like mammals would have low diversity but high counts, while a Group like Insects would have high diversity but lower counts (based on what I know about animal diversity and how I think body size will impact eDNA abundance). Notably, some Groups had a lot of diversity, and as I built the workshop and some of the figures I realised that having too many sequences made the data points difficult to differentiate. Eventually I trimmed the original data set so that each group had 500 randomly selected sequences. This made visualization easier, but forced me to re-work the example.\n\n\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ntop_group_counts &lt;- read.delim(\"https://raw.githubusercontent.com/tylermcinnes/visualization_day/refs/heads/main/data/eDNA_group_counts.tsv\")\n\n\nFamiliarise yourself with the data\nHere we will use some functions to check features of our data\n\ntop_group_counts |&gt; head()\n\n                 Name                                   CommonName TaxID Count\n1  Anas platyrhynchos                                 Mallard duck  8839   273\n2      Pavo cristatus Indian peafowl; common peafowl; blue peafowl  9049    33\n3 Carduelis carduelis                                    Goldfinch 37600     6\n4 Porphyrio melanotus                                       Pukeko 72013  1361\n5 Carduelis carduelis                                    Goldfinch 37600    20\n6           Zosterops                                    White eye 36298    13\n     Rank Group   Phylum Class         Order       Family     Genus\n1 species Birds Chordata  Aves  Anseriformes     Anatidae      Anas\n2 species Birds Chordata  Aves   Galliformes  Phasianidae      Pavo\n3 species Birds Chordata  Aves Passeriformes Fringillidae Carduelis\n4 species Birds Chordata  Aves    Gruiformes     Rallidae Porphyrio\n5 species Birds Chordata  Aves Passeriformes Fringillidae Carduelis\n6   genus Birds Chordata  Aves Passeriformes Zosteropidae Zosterops\n\ntop_group_counts |&gt; tail()\n\n                                Name CommonName  TaxID Count    Rank Group\n4995                            Nais Sludgeworm  74730   101   genus Worms\n4996                            Dero       Worm  66487    72   genus Worms\n4997 Cernosvitoviella aggtelekiensis       Worm 913639    13 species Worms\n4998              Octolasion lacteum       Worm 334871    36 species Worms\n4999          Lumbriculus variegatus  Blackworm  61662  3848 species Worms\n5000        Limnodrilus hoffmeisteri    Redworm  76587    19 species Worms\n       Phylum      Class            Order        Family            Genus\n4995 Annelida Clitellata       Tubificida      Naididae             Nais\n4996 Annelida Clitellata       Tubificida      Naididae             Dero\n4997 Annelida Clitellata     Enchytraeida Enchytraeidae Cernosvitoviella\n4998 Annelida Clitellata Crassiclitellata   Lumbricidae       Octolasion\n4999 Annelida Clitellata     Lumbriculida Lumbriculidae      Lumbriculus\n5000 Annelida Clitellata       Tubificida      Naididae      Limnodrilus\n\ntop_group_counts |&gt; class()\n\n[1] \"data.frame\"\n\n# Verify that all groups have 500 observations\ntop_group_counts |&gt; \n  group_by(Group) |&gt; \n  summarize(observations = n(), .groups = \"drop\") |&gt; \n  arrange(desc(observations)) |&gt; \n  head()\n\n# A tibble: 6 × 2\n  Group    observations\n  &lt;chr&gt;           &lt;int&gt;\n1 Birds             500\n2 Ciliates          500\n3 Diatoms           500\n4 Fish              500\n5 Insects           500\n6 Mammals           500",
    "crumbs": [
      "Building Better Plots"
    ]
  },
  {
    "objectID": "viz_4_building_better.html#initial-visualization",
    "href": "viz_4_building_better.html#initial-visualization",
    "title": "Building Better Plots",
    "section": "Initial visualization",
    "text": "Initial visualization\nIn this example we are looking at the variance in Count across the 10 groups. A very reasonable starting point for this type of visualization is the boxplot. We will use a basic boxplot to get a ‘sketch’ of what our visualization will look like, then we will test other geoms (plot types) and look at colours, titles, labels etc.,.\n\nBoxplot 101\nA basic boxplot:\n\nggplot(data = top_group_counts,\n  mapping = aes(x = Group, y = Count)) +\ngeom_boxplot()\n\n\n\n\n\n\n\n\nThis figure shows us the Count variable is highly skewed (the majority of values are small, with some values many fold higher).\nExercise: use the summary() function and look at the outcome for the Count variable. Note the min, mean, median and maximum value. More advanced R users could use the filter() and nrow() functions to ask how many rows have a Count value that is 10 times greater than the Count median (or ask this question using whatever functions you are familiar with).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ntop_group_counts |&gt; summary()\n\n     Name            CommonName            TaxID              Count        \n Length:5000        Length:5000        Min.   :    2706   Min.   :    5.0  \n Class :character   Class :character   1st Qu.:    9940   1st Qu.:   17.0  \n Mode  :character   Mode  :character   Median :   76587   Median :   51.0  \n                                       Mean   :  401972   Mean   :  421.9  \n                                       3rd Qu.:  238745   3rd Qu.:  197.2  \n                                       Max.   :10000288   Max.   :59445.0  \n     Rank              Group              Phylum             Class          \n Length:5000        Length:5000        Length:5000        Length:5000       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    Order              Family             Genus          \n Length:5000        Length:5000        Length:5000       \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n\n# Note the outputs, specifically that for Count. \n## Min = 5.0, Median = 51.0, Mean = 421.9, Max = 59445\n### This is a sign the data is skewed.\n\n# How many rows have a Count value that is more than 10 times greater than the median? \ntop_group_counts |&gt;\n  filter(Count &gt;= median(top_group_counts$Count)*10) |&gt;\n  nrow()\n\n[1] 681\n\n# This shows 681 elements are more than 10* higher than the median - very skewed.\n\n\n\n\nWe can address the issue of skewed data by taking the log of Count before we plot it.\n\nggplot(data = top_group_counts,\nmapping = aes(x = Group, y = log(Count))) +\ngeom_boxplot()\n\n\n\n\n\n\n\n\nDesign tip: It can often be worth representing boxplots on a horizontal, rather than vertical, distribution. Let’s switch our axes assignments:\n\nggplot(data = top_group_counts,\nmapping = aes(x = log(Count), y = Group)) +\ngeom_boxplot()\n\n\n\n\n\n\n\n\nGenerally we will find that sometimes switching the layout like this can provide us with clearer visualization, sometimes due to labels, sometimes for aesthetics. In this case, I think the group labels read much more clearly when stacked vertically on top of one another with the new layout. I encourage you to think about small changes like this when creating your visualizations.\n\n\nSort the data\nSorted data will be easier to interpret, with a lower cognitive load. Note: we should only sort data if there is no inherent structure - I don’t believe Mammals are inherently ‘higher’ than Fish, so I can sort these groups based on another feature. Sometimes we will have internal structure, either some kind of hierarchy or grouping (e.g., case vs control) and it is not appropriate to apply sorting.\nThis is where we need to bring in some of our data transformation skills. This is not the focus of this episode, so we won’t go into this, but we are using a function, fct_reorder(), to re-order the Group variable based on data from the Count variable, where Count is ordered based on median. The mutate() function can be used to create a new variable, or in this case rearrange an existing variable.\n(Disclaimer, building this workshop I knew what I wanted to do and used chatGPT to guide me through the process.)\n\ntop_group_counts_sorted &lt;- top_group_counts |&gt; \n  mutate(Group = fct_reorder(Group, Count, .fun = median))\n\nNow plot the sorted data:\n\nggplot(data = top_group_counts_sorted,\nmapping = aes(x = log(Count), y = Group)) +\ngeom_boxplot()\n\n\n\n\n\n\n\n\nThis is immediately clearer, with a considerably lower cognitive load. Easily observe that not only does the Fish group have a higher set of Counts, but it seems to break with the trend of the other Groups.\n\nOutliers\nIn boxplots, outliers are identified as values that are more than 1.5 times the interquartile range above or below the third and first quartile, respectively. In our data these outliers are reasonably numerous. This could be due to inherently variable data, due either to biological or technical reasons, site differences etc.,. Without wanting to make assumptions about the data we can experiment with visualizations and choose how we want to present this data.",
    "crumbs": [
      "Building Better Plots"
    ]
  },
  {
    "objectID": "viz_4_building_better.html#better-visualization",
    "href": "viz_4_building_better.html#better-visualization",
    "title": "Building Better Plots",
    "section": "Better visualization",
    "text": "Better visualization\nWe can now start to focus on the visualization aspects: control plot themes such as labels, titles, spacing. We can explore geoms, and variations on geoms, to determine the best way to present the data. Finally, we can add additional geoms or mappings to display more data.\n\nTheme\nTheme is a separate function that controls almost all visual elements of a ggplot. We can fine tune text elements (font size, shape, angle for axis text, create custom labels and titles), the legend (changing the position, setting a background, control the title and content), the ‘panel’ (panel is the background of the plot, in the above cases we have a grid), and many other features.\nA useful tip for working with ggplot is to save a set of basic features, such as the data, mapping, and theme to an object which can then be used for plotting with different geoms later. Notice below we use a slightly different format for writing our ggplot function - we omit “data =” and “mapping =”, since those calls are always required and used.\n\ngroupCounts &lt;- ggplot(top_group_counts_sorted, \n                      aes(x = log(Count), \n                          y = Group, \n                          colour = Group)) +\n  labs(title = \"eDNA counts vary across species groups\",\n       x = \"log(Count)\", \n       y = \"Group\") +\n  theme_minimal() +     \n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(size = 16, face = \"bold\", hjust = 0.5),\n    axis.title = element_text(size = 14),\n    axis.text = element_text(size = 10),\n    panel.grid = element_blank()\n  ) \n\n# Note, no plot is created yet. We will now use the groupCounts object in combination\n# with geom functions to create plots.\n\nWe can now trial different geoms to determine which type suits our visualization needs.\n\nExercise: use “groupCounts +” and your choice of a geom function to plot the data with the new theme.\nTest multiple different geoms to experience how easily we can test different visualizations.\n\n\n\n\n\n\nExamples\n\n\n\n\n\n\ngroupCounts + \n  geom_boxplot() \n\n\n\n\n\n\n\n\n\ngroupCounts + \n  geom_violin() \n\n\n\n\n\n\n\n\n\ngroupCounts + \n  geom_line()\n\n\n\n\n\n\n\n\n\ngroupCounts + \n  geom_point()\n\n\n\n\n\n\n\n\nYou will probably agree that some of these geoms are much more suitable than others!\n\n\n\n\n\nAlpha\nA particularly useful argument with the geom_point() function is alpha, which controls the opacity/transparency of the points. With reduced opacity we can see more clearly when points are placed on top of one another. It is less clear in this example because there are so many data points in a small space, but it’s usefulness will become more apparent shortly.\n\ngroupCounts + \n  geom_point(size = 3, \n              alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n\nCombining geoms\nGeoms can be combined to create more complex plots by layering one set of information over another.\n\ngroupCounts + \n  geom_boxplot(colour = \"gray\", outlier.alpha = 0) +\n  geom_point(size = 3, alpha = 0.2)\n\n\n\n\n\n\n\n\nNote that we’ve included a new argument in geom_boxplot(): “outlier.alpha = 0”. The boxplot geom normally adds points that are considered outliers (greater than 1.5 times the interquartile range above or below the 1st or 3rd quartile). Since these points are going to be represented by the geom_point() function, we need to set outlier.alpha to 0 so that they are functionally invisible (and are therefore not printed twice).\nWhile the above demonstrates the concept of combining geoms, it’s not very functional. The high density of our points on a straight line is creating a strong effect that is over-powering other visualizations. We can change the points from being on a single line to spaced apart with the geom_jitter() function.\n\ngroupCounts + \n  geom_jitter(size = 2, \n              alpha = 0.2, \n              width = 0.2)\n\n\n\n\n\n\n\n\n\ngroupCounts + \n  geom_boxplot(colour = \"gray40\", \n                outlier.alpha = 0) +\n  geom_jitter(size = 2, \n              alpha = 0.2, \n              width = 0.2)\n\n\n\n\n\n\n\n# gray40 is a darker gray that is not quite black. I had to test multiple \n# colours to find one that was clearly visible but not overwhelming.\n\n\n\n(Even) more geoms\nThere are some really cool things we can do by combining geoms, and here your imagination and creativity is (probably) the limiting factor rather than ggplot.\nLet’s look at how each of our Groups compares to the average (and in this case, we will use mean instead of the median we have been using with our box plots).\nFirst, we will visualize the Group means with the stat_summary function.\n\ngroupCounts +\n  geom_jitter(size = 2, \n              alpha = 0.2, \n              width = 0.2) +\n  stat_summary(fun = mean, \n                geom = \"point\", \n                size = 5, \n                colour = \"gray40\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking use of values\n\n\n\n\n\nIn the code above we have specified the size of the stat_summary point to be a fixed value (5). In almost all cases within ggplot, you can select dynamic values for arguments e.g., if we had a different number of observations per group, point size could reflect these differences. Making full use of all of these variables leads to very informative plots.\n\n\n\nNext, we can add the overall mean for these groups as a vertical line:\n\n# Calculate the average number of Counts across all samples:\ntop10_group_avg &lt;- top_group_counts_sorted |&gt; \n  summarize(t10_avg = mean(log(Count))) |&gt; \n  pull(t10_avg)\n\n# Plot:\ngroupCounts +\n  geom_vline(aes(xintercept = top10_group_avg), \n                  colour = \"gray50\", \n                  size = 0.6) +\n  stat_summary(colour = \"gray40\",\n                fun = mean, \n                geom = \"point\", \n                size = 5) +\n  geom_jitter(size = 2, \n              alpha = 0.2, \n              width = 0.2)\n\n\n\n\n\n\n\n\nGeom order matters! Note how the summary points (black) are obscured by the coloured points produced by geom_jitter()? We can switch the order so that the jitter points are drawn first, and the summary points placed on top.\n\ngroupCounts +\n  geom_jitter(size = 2, \n              alpha = 0.2, \n              width = 0.2) +\n  geom_vline(aes(xintercept = top10_group_avg), \n                  colour = \"gray50\", \n                  size = 0.6, \n                  alpha = 0.8) +\n  stat_summary(colour = \"gray50\",\n                fun = mean, \n                geom = \"point\", \n                size = 5, \n                alpha = 0.75)\n\n\n\n\n\n\n\n\nIf we are trying to highlight the difference between a Group mean and the global mean, this can be amplified by adding text to our image.\n\ngroupMeans &lt;- top_group_counts_sorted |&gt; group_by(Group) |&gt; summarise(groupMean = log(mean(Count))) |&gt; pull(groupMean)\n\ngroupCounts +\n  geom_jitter(size = 2, \n              alpha = 0.2, \n              width = 0.2) +\n  geom_vline(aes(xintercept = top10_group_avg), \n                  colour = \"gray50\", \n                  size = 0.6, \n                  alpha = 0.8) +\n  stat_summary(colour = \"gray60\",\n                fun = mean, \n                geom = \"point\", \n                size = 5, \n                alpha = 0.8) +\n  annotate(\n    \"text\", x = 7.8, y = 10, size = 2.8, color = \"gray20\", lineheight = .9,\n    label = glue::glue(\"Counts for fish are significantly higher\\nthan other groups (7.6)\")\n  ) +\n    annotate(\n    \"text\", x = 5.25, y = 1, size = 2.8, color = \"gray20\",\n    label = \"Total group average (4.2)\"\n  )\n\n\n\n\n\n\n\n\nThis isn’t necessarily something you would do in many types of plots, but is useful to draw attention to key points (often, e.g., a single gene of interest in a scatter plot).\n\n\nSaving a plot\nWe can run the code below to save a plot with a given set of dimensions. The default is inches, but we can set this to cm or pixels.\n\nggsave(\"boxplot.png\", height = 10, width = 8)",
    "crumbs": [
      "Building Better Plots"
    ]
  },
  {
    "objectID": "viz_4_building_better.html#summary",
    "href": "viz_4_building_better.html#summary",
    "title": "Building Better Plots",
    "section": "Summary",
    "text": "Summary\nBy saving some of the ggplot2 code into an object, we can quickly and easily iterate on a plot, letting us sketch out different plot types and styles. The theme function let’s us save a set of traits that we can use across all of our visualizations, giving us a cohesive look across a single document.\nGeom functions can be combined in different ways to build up a complex chart, while still sticking to our basic template.",
    "crumbs": [
      "Building Better Plots"
    ]
  }
]